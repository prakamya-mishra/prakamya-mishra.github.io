---
layout: post
comments: false
title: "STEPs-RL: Speech-Text Entanglement for Phonetically Sound Representation Learning"
date: 2020-11-23 00:00:00
tags: paper speech representation-learning
---

> **Abstract:** In this post, I will present a novel multi-modal deep neural network architecture that uses speech and text entanglement for learning phonetically sound spoken-word representations (STEPs-RL). STEPs-RL is trained in a supervised manner to predict the phonetic sequence of a target spoken-word using its contextual spoken word's speech and text, such that the model encodes its meaningful latent representations. Unlike existing work, I have used text along with speech for auditory representation learning to capture semantical and syntactical information along with the acoustic and temporal information.


<!--more-->

Speaking and listening are the most common ways in which humans convey and understand each other in daily conversations. Nowadays, the speech interface has also been widely integrated into many applications/devices like Siri, Google Assistant, and Alexa. These applications use speech recognition-based approaches to understand the spoken user queries. Like speech, the text is also a widely used medium in which people converse. Recent advances in language modeling and representation learning using deep learning approaches have proven to be very promising in understanding the actual meanings of the textual data, by capturing semantical, syntactical, and contextual relationships between the textual words in their corresponding learned fixed-size vector representations.

So in this paper, we propose a novel spoken-word representation learning approach called STEPs-RL that uses speech and text entanglement for learning phonetically sound spoken-word representations, which not only captures the acoustic and contextual features but also are semantically, syntactically, and phonetically sound. STEPs-RL is trained in a supervised manner such that the learned representations can capture the phonetic structure of the spoken-words along with their inter-word semantic, syntactic, and contextual relationships. We validated the proposed model by (1) evaluating semantical and syntactical relationships between the learned spoken-word representations on four widely used word similarity benchmark datasets, and comparing its performance with the textual word representations learned by Word2Vec & FastTexT (obtained using transcriptions), and (2) investigating the phonetical soundness of the generated vector space.

{: class="table-of-content"}
* TOC
{:toc}

## Model
![STEPs-RL Model Architecture]({{ '/assets/Blog/STEPs-RL-SpeechVec.png' | relative_url }})
{: style="width: 65%;" class="center"}
*Fig. 1. Illustration of the STEPs-RL model architecture.

## Experimental Setup

## Results

## Future




---

Cited as:
```
@article{prakamya2020LG,
  title   = "STEPs-RL: Speech-Text Entanglement for Phonetically Sound Representation Learning",
  author  = "Prakamya Mishra",
  journal = "prakamya-mishra.github.io/Blog",
  year    = "2020",
  url     = "https://prakamya-mishra.github.io/Blog/2020-11-23-STEPs-RL.html"
}
```

---
layout: post
comments: false
title: "STEPs-RL: Speech-Text Entanglement for Phonetically Sound Representation Learning"
date: 2020-11-23 00:00:00
tags: paper speech representation-learning
---

> **Abstract:** In this post, I will present a novel multi-modal deep neural network architecture that uses speech and text entanglement for learning phonetically sound spoken-word representations (STEPs-RL). STEPs-RL is trained in a supervised manner to predict the phonetic sequence of a target spoken-word using its contextual spoken word's speech and text, such that the model encodes its meaningful latent representations. Unlike existing work, I have used text along with speech for auditory representation learning to capture semantical and syntactical information along with the acoustic and temporal information.


<!--more-->

<!--Speaking and listening are the most common ways in which humans convey and understand each other in daily conversations. Nowadays, the speech interface has also been widely integrated into many applications/devices like Siri, Google Assistant, and Alexa. These applications use speech recognition-based approaches to understand the spoken user queries. Like speech, the text is also a widely used medium in which people converse. Recent advances in language modeling and representation learning using deep learning approaches have proven to be very promising in understanding the actual meanings of the textual data, by capturing semantical, syntactical, and contextual relationships between the textual words in their corresponding learned fixed-size vector representations.

So in this paper, we propose a novel spoken-word representation learning approach called STEPs-RL that uses speech and text entanglement for learning phonetically sound spoken-word representations, which not only captures the acoustic and contextual features but also are semantically, syntactically, and phonetically sound. STEPs-RL is trained in a supervised manner such that the learned representations can capture the phonetic structure of the spoken-words along with their inter-word semantic, syntactic, and contextual relationships. We validated the proposed model by (1) evaluating semantical and syntactical relationships between the learned spoken-word representations on four widely used word similarity benchmark datasets, and comparing its performance with the textual word representations learned by Word2Vec & FastTexT (obtained using transcriptions), and (2) investigating the phonetical soundness of the generated vector space.

{: class="table-of-content"}
* TOC
{:toc}

## Model
In this paper, we propose STEPs-RL: Speech-Text Entanglement for Phonetically Sound Representation Learning. STEPs-RL is a novel spoken-word representation learning approach which entangles speech and text based contextual information for learning phonetically sound spoken-word representations. The model architecture is shown in Figure 1. Given a target spoken-word, its left and right contextual spoken-words, along with the textual word embeddings of the corresponding spoken-words, the proposed model tries to learn a vector representation of the target spoken-word that not only captures the semantic-based, syntax-based and acoustic-based information but also captures the phonetic-based information. Here, a single spoken-word consists of a sequence of acoustic features Mel-frequency Cepstral Coefficients (MFCCs); Each of the spoken-word is padded with silence, so that they all consists of a sequence of \(n\) acoustic features.

![STEPs-RL Model Architecture]({{ '/assets/Blog/STEPs-RL-SpeechVec.png' | relative_url }})
{: style="width: 40%;" class="center"}
Fig. 1. Illustration of the STEPs-RL model architecture.
{: style="width: 50%;" class="center"}

Our approach uses Bidirectional-LSTM for capturing the contextual information. Bidirectional-LSTM (also known as Bi-LSTM), uses two LSTM networks to capture contextual information in opposite directions (forward and backward) of a sequence. The final hidden representations corresponding to the sequence tokens is generated by concatenating the hidden representations generated by both the LSTM networks.

STEPs-RL consist of three independent Bi-LSTM networks represented by to capture contextual information respectively from (1) The acoustic features of the left and right contextual spoken-words, (2) The acoustic features of the target spoken-word, and (3) The pre-trained textual word embeddings of the corresponding target spoken-word, left contextual spoken-words and right contextual spoken-words.

All the three Bi-LSTM networks generate a final hidden state representation corresponding to each timestamp, a final output of the corresponding forward LSTM network, and a final output of the corresponding backward LSTM network. The final forward and backward outputs of $$BiLSTM_{C}$$ & $$BiLSTM_{W}$$ are concatenated to generate $$f^C$$ & $$f^W$$ respectively, which will later act as context vectors during the entanglement of speech and text.

![STEPs-RL Phase 1]({{ '/assets/Blog/STEPs-RL-sec1.png' | relative_url }})
{: style="width: 40%;" class="center"}
Fig. 2. STEPs-RL Phase 1: Each of the individual Bi-LSTM captures contextual information.
{: style="width: 50%;" class="center"}

For intuition, $$f^C$$ represents the final contextual representation of the spoken-words present in context of the target spoken-word, and $$f^W$$ represents the final semantical and syntactical contextual representation of all the corresponding textual words. In other words, $$f^C$$ captures the acoustic/speech-based contextual information whereas $$f^W$$ captures the text-based contextual information. Both $$f^C$$ & $$f^W$$, are then used to entangle speech and text-based contextual information with the target spoken-word by generating new speech and text entangled bidirectional hidden state representations of the target spoken-word by generating attention scores using the hidden representations generated by $$BiLSTM_{T}$$.

![STEPs-RL Phase 2]({{ '/assets/Blog/STEPs-RL-sec2.png' | relative_url }})
{: style="width: 40%;" class="center"}
Fig. 3. STEPs-RL Phase 2: Speech \& Text entanglement with target spoken word.
{: style="width: 50%;" class="center"}

In the above figure, $$h^{T,C}$$ & $$h^{T,W}$$ represents the newly generated speech-entangled and text-entangled hidden representations respectively; $$\alpha_i^{T,C}$$ & $$\alpha_i^{T,W}$$ represents the speech-entangled and text-entangled attention scores respectively, corresponding to the $$i^{th}$$ timestamp of the hidden representations generated by $$BiLSTM_{T}$$. The attention scores $$\alpha_i^{T,C}$$ & $$\alpha_i^{T,W}$$ are generated by taking the dot product ($$\bullet$$) of each of the timestamps of $$h^T$$ with the context vectors $$f^C$$ & $$f^W$$ respectively.

![STEPs-RL Phase 3]({{ '/assets/Blog/STEPs-RL-sec3.png' | relative_url }})
{: style="width: 40%;" class="center"}
Fig. 4. STEPs-RL Phase 3: Latent representation learning
{: style="width: 50%;" class="center"}

Next, the proposed model uses the newly generated speech-entangled and text-entangled hidden representations, along with the original bidirectional hidden state representations of the target spoken-word, to generate a latent vector representation $$z$$ of the target spoken-word by stacking (illustrated in Figure 4) all these three hidden representations on top of each other and passing it through a simple encoder LSTM network $$\overrightarrow{LSTM_{encode}}$$.

To add more information about the speaker, the proposed model linearly combines the latent vector with an auxiliary vector to generate a new latent representation of the target spoken-word. This new latent representation is the one that the proposed model tries to learn. The auxiliary vector is a one-hot vector that consists of information related to the speaker's gender/dialect or both. Such an auxiliary vector was introduced because usually, the pronunciation of different words usually depends on the speaker's gender and dialect and hence can help learn phonetically sound spoken-word representations. 

Next, the proposed model uses a decoder LSTM network $$\overrightarrow{LSTM_{decode}}$$ to predict the sequence of phonetic symbols of the corresponding target spoken-word using the above generated latent representation of the target spoken-word $$z_{new}$$.

## Experimental Setup

## Results

## Future



---

Cited as:
```
@article{prakamya2020LG,
  title   = "STEPs-RL: Speech-Text Entanglement for Phonetically Sound Representation Learning",
  author  = "Prakamya Mishra",
  journal = "prakamya-mishra.github.io/Blog",
  year    = "2020",
  url     = "https://prakamya-mishra.github.io/Blog/2020-11-23-STEPs-RL.html"
}
```
-->
---
```
Post comming soon.
```
